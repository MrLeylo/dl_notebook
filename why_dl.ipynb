{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Why Deep Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have been interested in the AI field for the last years you have probably heard cool names such as Machine Learning, Deep Learning, Supervised and Unsupervised Learning, training... But do you really know what they mean? The goal of this Python Notebook is no more than giving a summary of why we are where we are in this field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Learning? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find plenty of people who talks about Machine Learning (which we will mention as ML from now on) but not all of them know exactly what it is referring to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML vs AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is ML the same than AI? Conceptually, not. AI is the science of developing systems able to perform given tasks. If I want to build a face recognizer, I must use AI. \n",
    "\n",
    "ML is just an approach to AI, which consists on developing a system to learn from collected data, so it can apply its knowledge to perform the given task. In our case think about a big collection of faces from two people from which it learns to recognize them. \n",
    "\n",
    "Ideally, there exist approaches to AI which do not necessarily include ML, such as hand-designed decision trees, which do not infer their knowledge from previous examples but directly from the programmers criterion. After reading that you will probably think that non-ML based AI is a deprecated and decrepit science but it is just an easy to understand example to begin with. In fact, there are non-ML approaches which are already known but still have to reach their peak, such as Reinforcement Learning (RL).\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems of ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML is commonly used to solve specific tasks. They can be grouped in some common types.\n",
    "\n",
    "A common and easy to understand type of tasks is *Classification*. It consists in assigning classes to instances. For example, one simple classification task is classifying between cats and dogs from a collection of photos. Inside this kind of problems we can find a huge variability, and scientists are still introducing issues to be solved. That could be the case of *fine-grained image recognition* or *few-shot learning*. I will not focus on those problems (in a general sense they mean classifyig between similar classes from slight differences and classifying from a few examples respectively), but I just want to remark that from the generic idea of classification we can find plenty of concrete situations which could make the problem harder and would require us to think in a more robust approach to face the new issues.\n",
    "\n",
    "Another kind of tasks is *Detection*, which could be seen as a binary classification with restrictions. It consists in finding where is some instance in the input data. For example, finding the *bounding box* (which is already a restriction) containing a face in an image.\n",
    "\n",
    "We also can make *Generation* tasks, which consist in generating some kind of data (e.g. a painting) from seen examples. This kind of tasks is more abstract to evaluate.\n",
    "\n",
    "One of the most common kind of tasks of ML is *Regression*. They consist in making some value prediction given input data. An example of this could be a temperature predictor for tomorrow, given some measures taken today.\n",
    "\n",
    "We can see how the nature of the problem that we try to solve can classify our task in some group, but there is another way to define the ML strategy we will follow to solve our problem depending on its nature. I am talking about *Supervised* and *Unsupervised* learning.\n",
    "\n",
    "In *Supervised learning* we show the system a set of samples and its *ground truth* (the solution of the problem for that sample), which depends on the problem (e.g for classification it tells the class of the sample, for detection it tells the location of the bounding box it should predict), and we want it to be able to solve the problem for new samples later. That means that we want it to learn patterns for each solution.\n",
    "\n",
    "In *Unsupervised learning* we just show data without annotations to the system, and the idea is that it finds some patterns on it, disregarding the real solution. Therefore, it just will find some data distribution and will learn to differentiate it without having any knowledge of what is it at all.\n",
    "\n",
    "In the following summary we will refer to *Supervised learning*, which will be enough to understand. We will just make a reference to unsupervised learning to verify how it fits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional ML scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The are plenty of traditional ML algorithms which may seem very different, but they can be summarized by a common scheme. See it on the image below.\n",
    "\n",
    "<img src=\"ml_scheme.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is the *Data acquisition*, which may seem obvious but is in practice one step with a huge cost. Depending on the application it could need not only the *data collection* substep, but also further actions such as *data formatting* or *data preprocessing*. For data formatting I refer to adapting it into the domain that we need, for example, in [this simple system](https://upcommons.upc.edu/bitstream/handle/2117/83252/TFG_IgnasiMas_MathematicalExpressionRecognition.pdf) that I developed to identify handwritten mathematical expressions I applied an adaptation of the data referred to the trace to image. We could even include image resampling if it is needed. In some cases, the data labelling (we will go on that later) also needs some formatting (e.g. representing classes by some kind of vector, tipically one-hot-vectors). For data preprocessing we could imagine an image which needs some kind of cleaning or to make things easy we could also imagine a simple binarization. \n",
    "\n",
    "After that we have a *dataset* (key concept), which basically is a collection of labelled data samples with homogeneous format for both data and label over the different samples.\n",
    "\n",
    "There is tipically one final substep on the *data acquisition* step, which is *data split*. The dataset is split in subsets. The 3 common splits are *training set*, *validation set* and *test set*. We will skip the reason to the following paragraphs.\n",
    "\n",
    "Note that there are 2 pipelines, which refer to the two main stages of a ML system. The one above is the *training stage* and refers to the stage where the system learns. That means that it finds the patterns of the data for making a decision. In the case of classification, it finds prototypes of each class and boundaries between them. In the case of regression, it finds the way how the input data influences on the final decision. Let's see it in more detail.\n",
    "\n",
    "In the training pipeline we find a *feature extraction* step. What does it mean? Let's see it in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Modify the following values which represent the image and feature space to see its effect\n",
    "a = 20\n",
    "b = 150\n",
    "\n",
    "a = min(max(a, 0), 255)\n",
    "b = min(max(b, 0), 255)\n",
    "\n",
    "plt.close()\n",
    "plt.clf()\n",
    "dummy_img = np.array([[a], [b]])\n",
    "fig1 = plt.figure(1)\n",
    "plt.imshow(dummy_img, cmap='gray')\n",
    "fig2 = plt.figure(2)\n",
    "plt.scatter(a, b)\n",
    "plt.xlim(0, 255)\n",
    "plt.ylim(0, 255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that dummy example where the input data is that 2-pixel image. Imagine it has a given label. We must define a feature space to represent the image. In a feature space, we represent each sample in an N-dimensional space where each feature is a dimension. In that case we could simply represent it as a 2D feature space where each pixel is a feature with values between 0 and 255 (its intensity). It is a simple example of some *feature extraction* from a given input.\n",
    "\n",
    "Let's add a little bit of difficulty. Imagine now a 2x2 image and now our feature space will be based on the X and Y derivatives. That is another 2D feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "# Modify the following values which represent the image and feature space to see its effect\n",
    "a = 20\n",
    "b = 150\n",
    "c = 2\n",
    "d = 30\n",
    "\n",
    "a = min(max(a, 0), 255)\n",
    "b = min(max(b, 0), 255)\n",
    "c = min(max(c, 0), 255)\n",
    "d = min(max(d, 0), 255)\n",
    "\n",
    "plt.close()\n",
    "plt.clf()\n",
    "dummy_img = np.array([[a, b], [c, d]])\n",
    "fig1 = plt.figure(1)\n",
    "plt.imshow(dummy_img, cmap='gray')\n",
    "fig2 = plt.figure(2)\n",
    "plt.scatter(c - a, d - b)\n",
    "plt.xlim(-255, 255)\n",
    "plt.ylim(-255, 255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in that type of pipeline we must define the feature space by hand. We have defined simple examples, but the decision could be harder for more complex examples, such a human face recognizer, where the features are in a higher dimensional space. As a human decision, that could lead us to errors, and require a lot of work of both reasoning and experimenting.\n",
    "\n",
    "For complex examples, often *descriptors* are used, where we can use some detector to find the features and some descriptor to represent them in some format to locate them in the feature space (e.g. for the face recognition example, we detect some common features on the face and then we store the representation).\n",
    "\n",
    "For the *data storage*, depending on the algorithm, we could find a class representation (e.g. an average per feature) or use the trained data somehow to prepare it for production.\n",
    "\n",
    "Now let's talk about the *testing stage*. It basically is the system running om production, which means applying the task over the input data on the *test set* or on new samples. In the case of the *test set*, where in the case of supervised learning we have labels, the system is evaluated.\n",
    "\n",
    "In more detail, the same *feature extraction* step than in the training stage is applied (it must get the same kind of representation). In the testing case, the representation is compared to the stored data from training to get a result. For example, in the case of pattern matching for classification the representation from the new sample could be compared to the trained samples or to some prototype of each class using some distance (e.g. MSE), or evaluated between some defined boundaries on the trained feature space. There are more robust approaches which make a more complex use of the feature space such as *Support Vector Machines* (*SVM*), but we will not focus on that.\n",
    "\n",
    "And now, remember what we told about the *data split* step? Just be patient, we will be ready to understand it after introducing the following concept!\n",
    "\n",
    "It is a common case that we have some bias in our collected data. For example, imagine a face detector trained with photos taken over the population of some given country. The detector could learn the features very fit to the trained faces. But now imagine that on production appear faces from another country. Since the traits can be completely different the features can be evaluating some incorrect features.\n",
    "\n",
    "Let's look how it is seen on a graph. The system learns a boundary which works almost perfectly for the trained samples but it is focusing in some irrelevant shapes from training data which differ completely to the ones found on production.\n",
    "\n",
    "<img src=\"overfitting.png\">\n",
    "\n",
    "That phenomenom is known as *overfitting*. The systems fits so tight to the trained data that it does not tolerate differences from new samples.\n",
    "\n",
    "Now we are prepared to know why we need *data split*. As mentioned, we use the *training set* on the *training stage*, and the *test set* on the *testing stage*. The goal of the *testing stage* is to evaluate the performance of the system on new data, which also means evaluating that it does not suffer from overfitting. Therefore, the *training* and *test* sets must contain completely different samples, since the evaluation must be on new data. If the results on the *test set* are poor, we can evaluate on *training set* samples. If in the training case we have much better results, we could be suffering from overfitting. Else, the system simply performs poorly and we should solve it somehow (redefining the algorithm, selecting other features, preprocessing the data in some other way...).\n",
    "\n",
    "And what about the *validation set*? It is a completely different set, which is also used to prevent overfitting. It is used on training to evaluate the system for making some modifications which are not directly related to the data. For example, imagine that we want to ponderate the feature space to give more importance to some feature. We could base that ponderation on the different evaluations done on the *validation set*.\n",
    "\n",
    "Although that is the ideal way of working, in some real scenarios one could want to use other settings, such as using all the data on the *training set* (e.g. if the amount is too low and one tends to think that the data is representative enough), but the system would be more likely to suffer overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional ML issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the system described in the previous section is trained good enough, it can run alone and achieve good results. But you may have note that a lot of human supervision and hand-crafting is involved in the process.\n",
    "\n",
    "The cost of the *data acquisition* depends on the problem. For example, we don't need to label the samples on the dataset for the unsupervised learning problems (just if we want to perform an evaluation), but for the previous scheme we always need to define the feature space, which is a difficult task.\n",
    "\n",
    "That could be solved if some system was able to learn the relevant features alone, and that is more or less approached by Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Deep Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have probably heard about Deep Learning(DL) lately. Knowing the theory explained before about ML, we are ready to understand what DL is exactly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DL vs ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is DL another concept than ML? Well, it is a set of techniques of ML. We could say that DL is an approach to ML as well as ML is an approach to AI.\n",
    "\n",
    "DL is characterized by using algorithms called *Neural Networks*. On that Notebook I will no go deep on that, but the idea of *Neural Networks* is given some input, apply different operations (as a simulation of neurons) at what is called *nodes* to create some responses. That responses are then given to another group of nodes, and that is repeated until the end, when somehow the final responses are combined to give some joint response that will make the system take a final decision. Each group of nodes is a layer.\n",
    "\n",
    "<img src=\"nn.jpg\">\n",
    "\n",
    "DL uses *Convolutional Neural Networks* (*CNN's*), where each node can be seen as a filter. Each filter is convolved over the input data (typically images) to get a repsonse (take a look to the theory of convolution if you are not familiarized). Therefore, the filters are patterns (shapes) that can be found on the image. Note that at the first label only primal patterns are found but on further layers it can find higher dimensional features, even getting a semantical sense in some cases. The different levels of the features are combined sometimes.\n",
    "\n",
    "The ideal *CNN's* have many layers in order to get different levels on abstraction for the features. That's why DL is *Deep*. \n",
    "\n",
    "But which filters does a *CNN* use? Well, that is what the DL system learns at the *training stage*.\n",
    "\n",
    "Given some initial filters it evaluates their performance and update them depending on it. That is the main idea.\n",
    "\n",
    "How is that performance evaluated? It uses a tool called the *Loss function*. It is a metric that compares the real label and the prediction of the different samples. The closer, the lower is the value of the *Loss*.\n",
    "\n",
    "As we have mentioned, the *CNN* in the *training stage* begins with some initial filters (there are different kind of initialization, but let's imagine a random one) and evaluates them with a samples from the *training set* (yes, we need *data split* again) called a *batch* using the Loss function. Let's modify the filters and make another iteration with different samples. The Loss function will now give a different result. We must repeat that iteratively.\n",
    "\n",
    "But wait a moment. How are we modifying the filters, randomly? Of course not. Filters are in fact, feature spaces through which the Loss function value is updating. Imagine a dummy *CNN* that just has one simple 2-pixel filter. The loss function can be imagined as a surface over a 2D plane, since it is a function that depends on two variables. Obviously, we cannot determine the whole surface, but we can sample some points on it and draw a trajectory. That trajectory can be seen as the gradient between each two consecutively sampled points. \n",
    "\n",
    "Now, note that ideally we want to find the optimal value for the filter. That is the lowest point of the surface, which is the lowest value of the *Loss function*. Intuitively, we can follow the gradient to find it right? That is exactly what the *CNN* does on the *training stage*. To update the filters, at each iteration it \"jumps\" in the direction of the gradient of the *Loss function*. How coarse or fine is this jump is determined by the *learning rate*. High learning rates will make the *CNN* approach faster to the optimal value when far, but will have more difficulty to find it in a finer scale, while low ones will be more accurate but slower to approach when far. It is usual to define hybrid learning rates that are higher at the beginning and become lower when they approach to the optimal value, and there are algorithms to define that.\n",
    "\n",
    "We have seen an exaple with just one 2-pixel filter, but that can be extended to any *CNN* using the appropiate N-dimensional feature space.\n",
    "\n",
    "There are many training strategies, such as freezing some layers and just updating the other ones and at some point swapping them.\n",
    "\n",
    "At the testing stage, the *CNN* just predicts the result given the inputs using the trained filters.\n",
    "\n",
    "As we have already seen, *data split* is also necessary for DL since the overfitting phenomenom is common for all ML. In the case of DL, the *validation set* is used to evaluate some parameters such as the *learning rate* (for further update) or the likelihood of the performance to have reached *convergence* (when the optimal value is some close that we can't note any improvement in any direction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DL with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that in the previous definition of a DL system we have gone from lower to higher abstraction level where we define our task with DL instances which have a semantical sense. A complete Neural Network has a lot of low level mathematical operations. When we define a new architecture we just want to add some layers, connections, modify them and just sometimes define new operations. Imagine that we had to implement all the mathematical operations for each architecture. Fortunately, in DL we have *frameworks*, which are sets of libraries with standard DL operations and instances. There are low level frameworks, which just give us the operations and let us implement the instances (e.g. layers). In high level frameworks, they already give us those instances and let us build architectures just adding and parametrizing them, as if we were playing with boxes.\n",
    "\n",
    "In this notebook we will focus on Keras, which is a high level framework which works over a backend framework (which can be both Theano and Tensorflow) for the low level mathematical stuff. In the following code block, we have already defined a CNN. Play with it. Add layers, modify its parameters. Keras has so much to see and it is not the point of this notebook to explain it, but if you want to experiment a little bit more look at its [online documentation](https://keras.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "first_layer (Conv2D)         (None, 45, 45, 3)         84        \n",
      "_________________________________________________________________\n",
      "pooling_layer_1 (MaxPooling2 (None, 22, 22, 3)         0         \n",
      "_________________________________________________________________\n",
      "second_layer (Conv2D)        (None, 4, 4, 3)           84        \n",
      "_________________________________________________________________\n",
      "pooling_layer_2 (MaxPooling2 (None, 1, 1, 3)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 4)                 16        \n",
      "=================================================================\n",
      "Total params: 184\n",
      "Trainable params: 184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\users\\leylo\\life_reborn\\curro\\notebooks\\venv\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Convolution2D, Input, Dense, Flatten\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.activations import sigmoid, softmax\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "def build_dummy_network():\n",
    "    \n",
    "    model_input = Input(shape=(224, 224, 3))\n",
    "\n",
    "    x = Convolution2D(filters=3, \n",
    "                      kernel_size=(3, 3),\n",
    "                      strides=(5, 5),\n",
    "                      padding='valid',\n",
    "                      activation=sigmoid,\n",
    "                      name='first_layer')(model_input)\n",
    "    \n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='pooling_layer_1')(x)\n",
    "    \n",
    "    x = Convolution2D(filters=3, \n",
    "                      kernel_size=(3, 3),\n",
    "                      strides=(5, 5),\n",
    "                      padding='valid',\n",
    "                      activation=softmax,\n",
    "                      name='second_layer')(x)\n",
    "    \n",
    "    x = MaxPooling2D((2, 2), strides=(3, 3), name='pooling_layer_2')(x)\n",
    "    \n",
    "    x = Flatten(name='flatten')(x)\n",
    "    \n",
    "    x = Dense(units=4, name='dense_layer', activation='softmax')(x)\n",
    "    \n",
    "    model = Model(input=[model_input], output=[x])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_dummy_network()\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train and evaluate this previous model. In the following example we do it with dummy data that we generate. I remark that the data is dummy, so don't attention to the score value, but it should be useful to understand a training and evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "40/40 [==============================] - 1s 29ms/step - loss: 1.4539\n",
      "Epoch 2/3\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 1.4270\n",
      "Epoch 3/3\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 1.3855\n",
      "20/20 [==============================] - 0s 3ms/step\n",
      "Score: \n",
      "1.3402401208877563\n"
     ]
    }
   ],
   "source": [
    "x_train = np.concatenate((np.zeros((20, 224, 224, 3)), \n",
    "                          np.ones((20, 224, 224, 3))),\n",
    "                         axis=0)\n",
    "y_train = keras.utils.to_categorical(np.concatenate((np.zeros((20, 1)), \n",
    "                                                     np.ones((20, 1))),\n",
    "                                                    axis=0), num_classes=4)\n",
    "x_test = np.concatenate((np.zeros((10, 224, 224, 3)), \n",
    "                          np.ones((10, 224, 224, 3))),\n",
    "                         axis=0)\n",
    "y_test = keras.utils.to_categorical(np.concatenate((np.zeros((10, 1)), \n",
    "                                                     np.ones((10, 1))),\n",
    "                                                    axis=0), num_classes=4)\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=3)\n",
    "score = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('Score: ')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does DL give us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spread of DL has been due to simply some circumstances. The current hardware of the GPUs make easy to process convolutions, so we currently have the optimal hardware for *CNN's*, plus the spread of the internet as a tool for science sharing among other things. All that has been the motivation of the new DL hype.\n",
    "\n",
    "Moreover, it improves some of the limitations mentioned about previous approaches. In contrast with traditional ML, DL lets us not worry about defining any feature space, since it defines it itself in the *training stage*. That's good because it releases us from some work on defining the most relevant features, but at the same time it can be dangerous if we don't take care, since we loose control of what we are doing.\n",
    "\n",
    "That is the reason why it has some detractors, because most of the community tend to just experiment with boxes without going low levela nd there is people against that.\n",
    "\n",
    "My personal opinion on that is that DL is a positive step in  ML (therefore in AI) as long as we know what we are doing.\n",
    "\n",
    "Currently, the state of the art on many tasks is achieved by DL approaches."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
